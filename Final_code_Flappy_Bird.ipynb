{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitted by \n",
    "1. Akanxa Jain (UIN: 671123766)\n",
    "2. Anupam Sinha (UIN: 675450883))\n",
    "3. Nimita Singh (UIN: 667522322))\n",
    "4. Sriram Rajan (UIN: 668195501)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidkhetawat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py:29: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\sidkhetawat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.CSRRD7HKRKC3T3YXA7VY7TAZGLSWDKW6.gfortran-win_amd64.dll\n",
      "C:\\Users\\sidkhetawat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "WARNING:tensorflow:From C:\\Users\\sidkhetawat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\sidkhetawat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\sidkhetawat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\sidkhetawat\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from saved_networks\\bird-dqn-10000\n",
      "Successfully loaded: saved_networks\\bird-dqn-10000\n",
      "TIMESTEP 1 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.157960e+01\n",
      "TIMESTEP 2 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.163240e+01\n",
      "TIMESTEP 3 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.163591e+01\n",
      "TIMESTEP 4 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.156261e+01\n",
      "TIMESTEP 5 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.158510e+01\n",
      "TIMESTEP 6 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.165276e+01\n",
      "TIMESTEP 7 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.175605e+01\n",
      "TIMESTEP 8 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.176058e+01\n",
      "TIMESTEP 9 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.183933e+01\n",
      "TIMESTEP 10 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.189136e+01\n",
      "TIMESTEP 11 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.189744e+01\n",
      "TIMESTEP 12 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.191882e+01\n",
      "TIMESTEP 13 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.196488e+01\n",
      "TIMESTEP 14 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.194368e+01\n",
      "TIMESTEP 15 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.203118e+01\n",
      "TIMESTEP 16 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.200202e+01\n",
      "TIMESTEP 17 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.197132e+01\n",
      "TIMESTEP 18 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.201188e+01\n",
      "TIMESTEP 19 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.198757e+01\n",
      "TIMESTEP 20 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.199101e+01\n",
      "TIMESTEP 21 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.204374e+01\n",
      "TIMESTEP 22 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.207244e+01\n",
      "TIMESTEP 23 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.209685e+01\n",
      "TIMESTEP 24 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.208336e+01\n",
      "TIMESTEP 25 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.204722e+01\n",
      "TIMESTEP 26 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.211442e+01\n",
      "TIMESTEP 27 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.215993e+01\n",
      "TIMESTEP 28 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.215815e+01\n",
      "TIMESTEP 29 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.226304e+01\n",
      "TIMESTEP 30 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.227605e+01\n",
      "TIMESTEP 31 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.241007e+01\n",
      "TIMESTEP 32 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.247205e+01\n",
      "TIMESTEP 33 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.246762e+01\n",
      "TIMESTEP 34 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.252032e+01\n",
      "TIMESTEP 35 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.245435e+01\n",
      "TIMESTEP 36 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.239314e+01\n",
      "TIMESTEP 37 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.243638e+01\n",
      "TIMESTEP 38 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.241569e+01\n",
      "TIMESTEP 39 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.244211e+01\n",
      "TIMESTEP 40 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.244466e+01\n",
      "TIMESTEP 41 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.243575e+01\n",
      "TIMESTEP 42 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.238974e+01\n",
      "TIMESTEP 43 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.241694e+01\n",
      "TIMESTEP 44 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.246460e+01\n",
      "TIMESTEP 45 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.253564e+01\n",
      "TIMESTEP 46 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.257547e+01\n",
      "TIMESTEP 47 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.258005e+01\n",
      "TIMESTEP 48 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.261315e+01\n",
      "TIMESTEP 49 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.260105e+01\n",
      "TIMESTEP 50 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.272288e+01\n",
      "TIMESTEP 51 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.271313e+01\n",
      "TIMESTEP 52 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.277586e+01\n",
      "TIMESTEP 53 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.278802e+01\n",
      "TIMESTEP 54 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.280654e+01\n",
      "TIMESTEP 55 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.288956e+01\n",
      "TIMESTEP 56 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.288545e+01\n",
      "TIMESTEP 57 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.293209e+01\n",
      "TIMESTEP 58 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.295526e+01\n",
      "TIMESTEP 59 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.295219e+01\n",
      "TIMESTEP 60 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 1 / Q_MAX 1.290299e+01\n",
      "TIMESTEP 61 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.227350e+01\n",
      "TIMESTEP 62 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.222493e+01\n",
      "TIMESTEP 63 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.217798e+01\n",
      "TIMESTEP 64 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.209106e+01\n",
      "TIMESTEP 65 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.206793e+01\n",
      "TIMESTEP 66 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.206480e+01\n",
      "TIMESTEP 67 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.213658e+01\n",
      "TIMESTEP 68 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.214306e+01\n",
      "TIMESTEP 69 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.219361e+01\n",
      "TIMESTEP 70 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.231281e+01\n",
      "TIMESTEP 71 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.241600e+01\n",
      "TIMESTEP 72 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.247061e+01\n",
      "TIMESTEP 73 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.247078e+01\n",
      "TIMESTEP 74 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.258949e+01\n",
      "TIMESTEP 75 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.259234e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 76 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.252130e+01\n",
      "TIMESTEP 77 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.244271e+01\n",
      "TIMESTEP 78 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.242091e+01\n",
      "TIMESTEP 79 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.246042e+01\n",
      "TIMESTEP 80 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.250624e+01\n",
      "TIMESTEP 81 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.250419e+01\n",
      "TIMESTEP 82 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.253268e+01\n",
      "TIMESTEP 83 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.253011e+01\n",
      "TIMESTEP 84 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.264443e+01\n",
      "TIMESTEP 85 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.268943e+01\n",
      "TIMESTEP 86 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.265617e+01\n",
      "TIMESTEP 87 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.276342e+01\n",
      "TIMESTEP 88 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.277865e+01\n",
      "TIMESTEP 89 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.287796e+01\n",
      "TIMESTEP 90 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.291214e+01\n",
      "TIMESTEP 91 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.297732e+01\n",
      "TIMESTEP 92 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.296798e+01\n",
      "TIMESTEP 93 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.308170e+01\n",
      "TIMESTEP 94 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.302815e+01\n",
      "TIMESTEP 95 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.304775e+01\n",
      "TIMESTEP 96 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 1 / Q_MAX 1.307372e+01\n",
      "TIMESTEP 97 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.221734e+01\n",
      "TIMESTEP 98 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.205121e+01\n",
      "TIMESTEP 99 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.206925e+01\n",
      "TIMESTEP 100 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.218389e+01\n",
      "TIMESTEP 101 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.212529e+01\n",
      "TIMESTEP 102 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.222534e+01\n",
      "TIMESTEP 103 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.219727e+01\n",
      "TIMESTEP 104 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.218272e+01\n",
      "TIMESTEP 105 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.218160e+01\n",
      "TIMESTEP 106 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.230517e+01\n",
      "TIMESTEP 107 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.229743e+01\n",
      "TIMESTEP 108 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.232834e+01\n",
      "TIMESTEP 109 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.235406e+01\n",
      "TIMESTEP 110 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.251628e+01\n",
      "TIMESTEP 111 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.247352e+01\n",
      "TIMESTEP 112 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.261469e+01\n",
      "TIMESTEP 113 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.247018e+01\n",
      "TIMESTEP 114 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.243036e+01\n",
      "TIMESTEP 115 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.248284e+01\n",
      "TIMESTEP 116 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.243765e+01\n",
      "TIMESTEP 117 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.239709e+01\n",
      "TIMESTEP 118 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.240610e+01\n",
      "TIMESTEP 119 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.241424e+01\n",
      "TIMESTEP 120 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.249242e+01\n",
      "TIMESTEP 121 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.254550e+01\n",
      "TIMESTEP 122 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.258408e+01\n",
      "TIMESTEP 123 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.261718e+01\n",
      "TIMESTEP 124 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.267072e+01\n",
      "TIMESTEP 125 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.265591e+01\n",
      "TIMESTEP 126 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.276409e+01\n",
      "TIMESTEP 127 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.279757e+01\n",
      "TIMESTEP 128 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.282726e+01\n",
      "TIMESTEP 129 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.287084e+01\n",
      "TIMESTEP 130 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.295896e+01\n",
      "TIMESTEP 131 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.294700e+01\n",
      "TIMESTEP 132 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.295330e+01\n",
      "TIMESTEP 133 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.294501e+01\n",
      "TIMESTEP 134 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 1 / Q_MAX 1.300141e+01\n",
      "TIMESTEP 135 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.210944e+01\n",
      "TIMESTEP 136 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.211015e+01\n",
      "TIMESTEP 137 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.211213e+01\n",
      "TIMESTEP 138 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.208886e+01\n",
      "TIMESTEP 139 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.213800e+01\n",
      "TIMESTEP 140 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.218750e+01\n",
      "TIMESTEP 141 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.223084e+01\n",
      "TIMESTEP 142 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.225600e+01\n",
      "TIMESTEP 143 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.228514e+01\n",
      "TIMESTEP 144 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.222313e+01\n",
      "TIMESTEP 145 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.225484e+01\n",
      "TIMESTEP 146 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.237211e+01\n",
      "TIMESTEP 147 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.232211e+01\n",
      "TIMESTEP 148 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.240267e+01\n",
      "TIMESTEP 149 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.233502e+01\n",
      "TIMESTEP 150 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.231479e+01\n",
      "TIMESTEP 151 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.232620e+01\n",
      "TIMESTEP 152 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.239143e+01\n",
      "TIMESTEP 153 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.237205e+01\n",
      "TIMESTEP 154 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.238967e+01\n",
      "TIMESTEP 155 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.238902e+01\n",
      "TIMESTEP 156 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.243750e+01\n",
      "TIMESTEP 157 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.255742e+01\n",
      "TIMESTEP 158 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.258578e+01\n",
      "TIMESTEP 159 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.265479e+01\n",
      "TIMESTEP 160 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.268345e+01\n",
      "TIMESTEP 161 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.272516e+01\n",
      "TIMESTEP 162 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.275891e+01\n",
      "TIMESTEP 163 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.275842e+01\n",
      "TIMESTEP 164 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.280010e+01\n",
      "TIMESTEP 165 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.285469e+01\n",
      "TIMESTEP 166 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.288028e+01\n",
      "TIMESTEP 167 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.290346e+01\n",
      "TIMESTEP 168 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.287818e+01\n",
      "TIMESTEP 169 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.301483e+01\n",
      "TIMESTEP 170 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 1 / Q_MAX 1.298867e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 171 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.202336e+01\n",
      "TIMESTEP 172 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.204830e+01\n",
      "TIMESTEP 173 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.205529e+01\n",
      "TIMESTEP 174 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.209725e+01\n",
      "TIMESTEP 175 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.208516e+01\n",
      "TIMESTEP 176 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.208009e+01\n",
      "TIMESTEP 177 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.206628e+01\n",
      "TIMESTEP 178 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.206438e+01\n",
      "TIMESTEP 179 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.205874e+01\n",
      "TIMESTEP 180 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.213271e+01\n",
      "TIMESTEP 181 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.214605e+01\n",
      "TIMESTEP 182 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.218868e+01\n",
      "TIMESTEP 183 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.227319e+01\n",
      "TIMESTEP 184 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.236193e+01\n",
      "TIMESTEP 185 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.239569e+01\n",
      "TIMESTEP 186 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.239115e+01\n",
      "TIMESTEP 187 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.236656e+01\n",
      "TIMESTEP 188 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.241048e+01\n",
      "TIMESTEP 189 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.245787e+01\n",
      "TIMESTEP 190 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.253238e+01\n",
      "TIMESTEP 191 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.262057e+01\n",
      "TIMESTEP 192 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.261467e+01\n",
      "TIMESTEP 193 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.256343e+01\n",
      "TIMESTEP 194 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.256384e+01\n",
      "TIMESTEP 195 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.267184e+01\n",
      "TIMESTEP 196 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.271644e+01\n",
      "TIMESTEP 197 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.277262e+01\n",
      "TIMESTEP 198 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.279386e+01\n",
      "TIMESTEP 199 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.282593e+01\n",
      "TIMESTEP 200 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.288251e+01\n",
      "TIMESTEP 201 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.287922e+01\n",
      "TIMESTEP 202 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.289956e+01\n",
      "TIMESTEP 203 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.290523e+01\n",
      "TIMESTEP 204 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.293683e+01\n",
      "TIMESTEP 205 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.293726e+01\n",
      "TIMESTEP 206 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.305464e+01\n",
      "TIMESTEP 207 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.299831e+01\n",
      "TIMESTEP 208 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 1 / Q_MAX 1.301541e+01\n",
      "TIMESTEP 209 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.216325e+01\n",
      "TIMESTEP 210 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.222026e+01\n",
      "TIMESTEP 211 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.215460e+01\n",
      "TIMESTEP 212 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.213631e+01\n",
      "TIMESTEP 213 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.214797e+01\n",
      "TIMESTEP 214 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.222240e+01\n",
      "TIMESTEP 215 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.218642e+01\n",
      "TIMESTEP 216 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.217157e+01\n",
      "TIMESTEP 217 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.219528e+01\n",
      "TIMESTEP 218 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.222389e+01\n",
      "TIMESTEP 219 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.226984e+01\n",
      "TIMESTEP 220 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.232329e+01\n",
      "TIMESTEP 221 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.236226e+01\n",
      "TIMESTEP 222 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.241617e+01\n",
      "TIMESTEP 223 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.242172e+01\n",
      "TIMESTEP 224 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.239457e+01\n",
      "TIMESTEP 225 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.233140e+01\n",
      "TIMESTEP 226 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.237789e+01\n",
      "TIMESTEP 227 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.235698e+01\n",
      "TIMESTEP 228 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.240457e+01\n",
      "TIMESTEP 229 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.239447e+01\n",
      "TIMESTEP 230 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.249570e+01\n",
      "TIMESTEP 231 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.246820e+01\n",
      "TIMESTEP 232 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.250892e+01\n",
      "TIMESTEP 233 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.261361e+01\n",
      "TIMESTEP 234 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.278984e+01\n",
      "TIMESTEP 235 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.274402e+01\n",
      "TIMESTEP 236 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.279682e+01\n",
      "TIMESTEP 237 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.281396e+01\n",
      "TIMESTEP 238 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.287057e+01\n",
      "TIMESTEP 239 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.284478e+01\n",
      "TIMESTEP 240 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.294210e+01\n",
      "TIMESTEP 241 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.295936e+01\n",
      "TIMESTEP 242 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.297271e+01\n",
      "TIMESTEP 243 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.298175e+01\n",
      "TIMESTEP 244 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 1 / Q_MAX 1.300211e+01\n",
      "TIMESTEP 245 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.214569e+01\n",
      "TIMESTEP 246 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.219795e+01\n",
      "TIMESTEP 247 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.222085e+01\n",
      "TIMESTEP 248 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.215264e+01\n",
      "TIMESTEP 249 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.213730e+01\n",
      "TIMESTEP 250 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.216625e+01\n",
      "TIMESTEP 251 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.223575e+01\n",
      "TIMESTEP 252 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.218102e+01\n",
      "TIMESTEP 253 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.220265e+01\n",
      "TIMESTEP 254 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.224478e+01\n",
      "TIMESTEP 255 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.234405e+01\n",
      "TIMESTEP 256 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.246299e+01\n",
      "TIMESTEP 257 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.252601e+01\n",
      "TIMESTEP 258 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.249699e+01\n",
      "TIMESTEP 259 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.253344e+01\n",
      "TIMESTEP 260 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.253036e+01\n",
      "TIMESTEP 261 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.248532e+01\n",
      "TIMESTEP 262 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.243764e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 263 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.244209e+01\n",
      "TIMESTEP 264 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.254728e+01\n",
      "TIMESTEP 265 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.253194e+01\n",
      "TIMESTEP 266 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.247675e+01\n",
      "TIMESTEP 267 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.245916e+01\n",
      "TIMESTEP 268 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.244998e+01\n",
      "TIMESTEP 269 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.249989e+01\n",
      "TIMESTEP 270 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.253437e+01\n",
      "TIMESTEP 271 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.258267e+01\n",
      "TIMESTEP 272 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.266821e+01\n",
      "TIMESTEP 273 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.271201e+01\n",
      "TIMESTEP 274 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.274667e+01\n",
      "TIMESTEP 275 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.284375e+01\n",
      "TIMESTEP 276 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.288449e+01\n",
      "TIMESTEP 277 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.287640e+01\n",
      "TIMESTEP 278 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.294531e+01\n",
      "TIMESTEP 279 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.295563e+01\n",
      "TIMESTEP 280 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.290324e+01\n",
      "TIMESTEP 281 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.288233e+01\n",
      "TIMESTEP 282 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 1 / Q_MAX 1.293035e+01\n",
      "TIMESTEP 283 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.217913e+01\n",
      "TIMESTEP 284 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.203875e+01\n",
      "TIMESTEP 285 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.205149e+01\n",
      "TIMESTEP 286 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.209623e+01\n",
      "TIMESTEP 287 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.204600e+01\n",
      "TIMESTEP 288 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.215282e+01\n",
      "TIMESTEP 289 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.215171e+01\n",
      "TIMESTEP 290 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.214558e+01\n",
      "TIMESTEP 291 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.220844e+01\n",
      "TIMESTEP 292 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.223876e+01\n",
      "TIMESTEP 293 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.230802e+01\n",
      "TIMESTEP 294 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.235793e+01\n",
      "TIMESTEP 295 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.241598e+01\n",
      "TIMESTEP 296 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.240844e+01\n",
      "TIMESTEP 297 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.244690e+01\n",
      "TIMESTEP 298 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.247948e+01\n",
      "TIMESTEP 299 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.249101e+01\n",
      "TIMESTEP 300 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.252849e+01\n",
      "TIMESTEP 301 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.248081e+01\n",
      "TIMESTEP 302 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.246318e+01\n",
      "TIMESTEP 303 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.244412e+01\n",
      "TIMESTEP 304 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.250655e+01\n",
      "TIMESTEP 305 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.246316e+01\n",
      "TIMESTEP 306 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.248202e+01\n",
      "TIMESTEP 307 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.258564e+01\n",
      "TIMESTEP 308 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.272180e+01\n",
      "TIMESTEP 309 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.267141e+01\n",
      "TIMESTEP 310 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.276981e+01\n",
      "TIMESTEP 311 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.280234e+01\n",
      "TIMESTEP 312 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.288728e+01\n",
      "TIMESTEP 313 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.292001e+01\n",
      "TIMESTEP 314 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.293545e+01\n",
      "TIMESTEP 315 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.296246e+01\n",
      "TIMESTEP 316 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.296001e+01\n",
      "TIMESTEP 317 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.298776e+01\n",
      "TIMESTEP 318 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 1 / Q_MAX 1.303013e+01\n",
      "TIMESTEP 319 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.213939e+01\n",
      "TIMESTEP 320 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.219844e+01\n",
      "TIMESTEP 321 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.220862e+01\n",
      "TIMESTEP 322 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.210767e+01\n",
      "TIMESTEP 323 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.213268e+01\n",
      "TIMESTEP 324 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.220121e+01\n",
      "TIMESTEP 325 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.217275e+01\n",
      "TIMESTEP 326 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.214856e+01\n",
      "TIMESTEP 327 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.218632e+01\n",
      "TIMESTEP 328 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.223153e+01\n",
      "TIMESTEP 329 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.228302e+01\n",
      "TIMESTEP 330 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.235107e+01\n",
      "TIMESTEP 331 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.238008e+01\n",
      "TIMESTEP 332 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.239795e+01\n",
      "TIMESTEP 333 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.244328e+01\n",
      "TIMESTEP 334 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.247915e+01\n",
      "TIMESTEP 335 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.248330e+01\n",
      "TIMESTEP 336 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.243306e+01\n",
      "TIMESTEP 337 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.248331e+01\n",
      "TIMESTEP 338 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.250112e+01\n",
      "TIMESTEP 339 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.248148e+01\n",
      "TIMESTEP 340 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.254524e+01\n",
      "TIMESTEP 341 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.249937e+01\n",
      "TIMESTEP 342 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.252404e+01\n",
      "TIMESTEP 343 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.256596e+01\n",
      "TIMESTEP 344 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.260018e+01\n",
      "TIMESTEP 345 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.263646e+01\n",
      "TIMESTEP 346 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.269241e+01\n",
      "TIMESTEP 347 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.271196e+01\n",
      "TIMESTEP 348 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.281313e+01\n",
      "TIMESTEP 349 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.288001e+01\n",
      "TIMESTEP 350 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.289456e+01\n",
      "TIMESTEP 351 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.294857e+01\n",
      "TIMESTEP 352 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.300072e+01\n",
      "TIMESTEP 353 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.303014e+01\n",
      "TIMESTEP 354 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.297283e+01\n",
      "TIMESTEP 355 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.297896e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 356 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 1 / Q_MAX 1.302122e+01\n",
      "TIMESTEP 357 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.214833e+01\n",
      "TIMESTEP 358 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.219724e+01\n",
      "TIMESTEP 359 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.213615e+01\n",
      "TIMESTEP 360 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.214641e+01\n",
      "TIMESTEP 361 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.212284e+01\n",
      "TIMESTEP 362 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.221924e+01\n",
      "TIMESTEP 363 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.227929e+01\n",
      "TIMESTEP 364 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.225920e+01\n",
      "TIMESTEP 365 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.229481e+01\n",
      "TIMESTEP 366 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.231610e+01\n",
      "TIMESTEP 367 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.230542e+01\n",
      "TIMESTEP 368 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.239025e+01\n",
      "TIMESTEP 369 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.243289e+01\n",
      "TIMESTEP 370 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.238797e+01\n",
      "TIMESTEP 371 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.236465e+01\n",
      "TIMESTEP 372 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.240465e+01\n",
      "TIMESTEP 373 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.244457e+01\n",
      "TIMESTEP 374 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.241644e+01\n",
      "TIMESTEP 375 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.238739e+01\n",
      "TIMESTEP 376 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.234139e+01\n",
      "TIMESTEP 377 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.252608e+01\n",
      "TIMESTEP 378 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.235490e+01\n",
      "TIMESTEP 379 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.250982e+01\n",
      "TIMESTEP 380 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.247989e+01\n",
      "TIMESTEP 381 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.252337e+01\n",
      "TIMESTEP 382 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.246224e+01\n",
      "TIMESTEP 383 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.260549e+01\n",
      "TIMESTEP 384 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.263160e+01\n",
      "TIMESTEP 385 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.265002e+01\n",
      "TIMESTEP 386 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.273557e+01\n",
      "TIMESTEP 387 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.275205e+01\n",
      "TIMESTEP 388 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.282094e+01\n",
      "TIMESTEP 389 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.288328e+01\n",
      "TIMESTEP 390 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.292865e+01\n",
      "TIMESTEP 391 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.291074e+01\n",
      "TIMESTEP 392 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 1 / Q_MAX 1.287947e+01\n",
      "TIMESTEP 393 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.193157e+01\n",
      "TIMESTEP 394 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.180395e+01\n",
      "TIMESTEP 395 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.208031e+01\n",
      "TIMESTEP 396 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.211886e+01\n",
      "TIMESTEP 397 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.214862e+01\n",
      "TIMESTEP 398 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.217128e+01\n",
      "TIMESTEP 399 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.212940e+01\n",
      "TIMESTEP 400 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.214641e+01\n",
      "TIMESTEP 401 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.214137e+01\n",
      "TIMESTEP 402 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.223450e+01\n",
      "TIMESTEP 403 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.229256e+01\n",
      "TIMESTEP 404 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.232358e+01\n",
      "TIMESTEP 405 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.235531e+01\n",
      "TIMESTEP 406 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.237871e+01\n",
      "TIMESTEP 407 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.239258e+01\n",
      "TIMESTEP 408 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.242375e+01\n",
      "TIMESTEP 409 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.239847e+01\n",
      "TIMESTEP 410 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.240171e+01\n",
      "TIMESTEP 411 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.245335e+01\n",
      "TIMESTEP 412 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.249377e+01\n",
      "TIMESTEP 413 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.254191e+01\n",
      "TIMESTEP 414 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.253096e+01\n",
      "TIMESTEP 415 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.253010e+01\n",
      "TIMESTEP 416 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.250646e+01\n",
      "TIMESTEP 417 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.260075e+01\n",
      "TIMESTEP 418 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.257283e+01\n",
      "TIMESTEP 419 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.263754e+01\n",
      "TIMESTEP 420 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.276338e+01\n",
      "TIMESTEP 421 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.273432e+01\n",
      "TIMESTEP 422 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.275311e+01\n",
      "TIMESTEP 423 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.283300e+01\n",
      "TIMESTEP 424 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.282210e+01\n",
      "TIMESTEP 425 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.290043e+01\n",
      "TIMESTEP 426 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.291540e+01\n",
      "TIMESTEP 427 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.288538e+01\n",
      "TIMESTEP 428 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.288956e+01\n",
      "TIMESTEP 429 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.297025e+01\n",
      "TIMESTEP 430 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 1 / Q_MAX 1.299135e+01\n",
      "TIMESTEP 431 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.211913e+01\n",
      "TIMESTEP 432 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.208678e+01\n",
      "TIMESTEP 433 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.204946e+01\n",
      "TIMESTEP 434 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.209004e+01\n",
      "TIMESTEP 435 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.211226e+01\n",
      "TIMESTEP 436 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.218030e+01\n",
      "TIMESTEP 437 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.216449e+01\n",
      "TIMESTEP 438 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.216608e+01\n",
      "TIMESTEP 439 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.215462e+01\n",
      "TIMESTEP 440 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.205363e+01\n",
      "TIMESTEP 441 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.226995e+01\n",
      "TIMESTEP 442 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.213829e+01\n",
      "TIMESTEP 443 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.223371e+01\n",
      "TIMESTEP 444 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.226847e+01\n",
      "TIMESTEP 445 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.237535e+01\n",
      "TIMESTEP 446 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.238754e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 447 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.238104e+01\n",
      "TIMESTEP 448 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.242893e+01\n",
      "TIMESTEP 449 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.237900e+01\n",
      "TIMESTEP 450 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.240842e+01\n",
      "TIMESTEP 451 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.243232e+01\n",
      "TIMESTEP 452 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.244538e+01\n",
      "TIMESTEP 453 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.247394e+01\n",
      "TIMESTEP 454 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.253656e+01\n",
      "TIMESTEP 455 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.253870e+01\n",
      "TIMESTEP 456 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.260165e+01\n",
      "TIMESTEP 457 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX 1.253287e+01\n",
      "TIMESTEP 458 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX 1.275865e+01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d23bfe366e07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-d23bfe366e07>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m     \u001b[0mplayGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-d23bfe366e07>\u001b[0m in \u001b[0;36mplayGame\u001b[1;34m()\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreadout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_fc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m     \u001b[0mtrainNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreadout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_fc1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-d23bfe366e07>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[1;34m(s, readout, h_fc1, sess)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;31m# run the selected action and observe next state and reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0mx_t1_colored\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[0mx_t1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_t1_colored\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_t1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_t1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTHRESH_BINARY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\IDS_576_team2_TermProject_FlappyBird\\game\\wrapped_flappy_bird.py\u001b[0m in \u001b[0;36mframe_step\u001b[1;34m(self, input_actions)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mimage_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurfarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_surface\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mFPSCLOCK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         \u001b[1;31m#print self.upperPipes[0]['y'] + PIPE_HEIGHT - int(BASEY * 0.2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimage_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.append(\"game/\")\n",
    "import wrapped_flappy_bird as game\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "GAME = 'bird' # the name of the game being played for log files\n",
    "ACTIONS = 2 # number of valid actions\n",
    "GAMMA = 0.99 # decay rate of past observations\n",
    "OBSERVE = 100000. # timesteps to observe before training\n",
    "EXPLORE = 2000000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.0001 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
    "\n",
    "def createNetwork():\n",
    "    # network weights\n",
    "    W_conv1 = weight_variable([8, 8, 4, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "\n",
    "    W_conv2 = weight_variable([4, 4, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    W_conv3 = weight_variable([3, 3, 64, 64])\n",
    "    b_conv3 = bias_variable([64])\n",
    "\n",
    "    W_fc1 = weight_variable([1600, 512])\n",
    "    b_fc1 = bias_variable([512])\n",
    "\n",
    "    W_fc2 = weight_variable([512, ACTIONS])\n",
    "    b_fc2 = bias_variable([ACTIONS])\n",
    "\n",
    "    # input layer\n",
    "    s = tf.placeholder(\"float\", [None, 80, 80, 4])\n",
    "\n",
    "    # hidden layers\n",
    "    h_conv1 = tf.nn.relu(conv2d(s, W_conv1, 4) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 2) + b_conv2)\n",
    "    #h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1) + b_conv3)\n",
    "    #h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "    #h_pool3_flat = tf.reshape(h_pool3, [-1, 256])\n",
    "    h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])\n",
    "\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # readout layer\n",
    "    readout = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "\n",
    "    return s, readout, h_fc1\n",
    "\n",
    "def trainNetwork(s, readout, h_fc1, sess):\n",
    "    # define the cost function\n",
    "    a = tf.placeholder(\"float\", [None, ACTIONS])\n",
    "    y = tf.placeholder(\"float\", [None])\n",
    "    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)\n",
    "    cost = tf.reduce_mean(tf.square(y - readout_action))\n",
    "    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)\n",
    "\n",
    "    # open up a game state to communicate with emulator\n",
    "    game_state = game.GameState()\n",
    "\n",
    "    # store the previous observations in replay memory\n",
    "    D = deque()\n",
    "\n",
    "    # printing\n",
    "    a_file = open(\"logs_\" + GAME + \"/readout.txt\", 'w')\n",
    "    h_file = open(\"logs_\" + GAME + \"/hidden.txt\", 'w')\n",
    "\n",
    "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] = 1\n",
    "    x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
    "    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "\n",
    "    # saving and loading networks\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    checkpoint = tf.train.get_checkpoint_state(\"saved_networks\")\n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Could not find old network weights\")\n",
    "\n",
    "    # start training\n",
    "    epsilon = INITIAL_EPSILON\n",
    "    t = 0\n",
    "    while \"flappy bird\" != \"angry bird\":\n",
    "        # choose an action epsilon greedily\n",
    "        readout_t = readout.eval(feed_dict={s : [s_t]})[0]\n",
    "        a_t = np.zeros([ACTIONS])\n",
    "        action_index = 0\n",
    "        if t % FRAME_PER_ACTION == 0:\n",
    "            if random.random() <= epsilon:\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[random.randrange(ACTIONS)] = 1\n",
    "            else:\n",
    "                action_index = np.argmax(readout_t)\n",
    "                a_t[action_index] = 1\n",
    "        else:\n",
    "            a_t[0] = 1 # do nothing\n",
    "\n",
    "        # scale down epsilon\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "        # run the selected action and observe next state and reward\n",
    "        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n",
    "        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "        ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)\n",
    "        x_t1 = np.reshape(x_t1, (80, 80, 1))\n",
    "        #s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)\n",
    "\n",
    "        # store the transition in D\n",
    "        D.append((s_t, a_t, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        # only train if done observing\n",
    "        if t > OBSERVE:\n",
    "            # sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "\n",
    "            # get the batch variables\n",
    "            s_j_batch = [d[0] for d in minibatch]\n",
    "            a_batch = [d[1] for d in minibatch]\n",
    "            r_batch = [d[2] for d in minibatch]\n",
    "            s_j1_batch = [d[3] for d in minibatch]\n",
    "\n",
    "            y_batch = []\n",
    "            readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})\n",
    "            for i in range(0, len(minibatch)):\n",
    "                terminal = minibatch[i][4]\n",
    "                # if terminal, only equals reward\n",
    "                if terminal:\n",
    "                    y_batch.append(r_batch[i])\n",
    "                else:\n",
    "                    y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
    "\n",
    "            # perform gradient step\n",
    "            train_step.run(feed_dict = {\n",
    "                y : y_batch,\n",
    "                a : a_batch,\n",
    "                s : s_j_batch}\n",
    "            )\n",
    "\n",
    "        # update the old values\n",
    "        s_t = s_t1\n",
    "        t += 1\n",
    "\n",
    "        # save progress every 10000 iterations\n",
    "        if t % 10000 == 0:\n",
    "            saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)\n",
    "\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
    "            \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
    "            \"/ Q_MAX %e\" % np.max(readout_t))\n",
    "        # write info to files\n",
    "        '''\n",
    "        if t % 10000 <= 100:\n",
    "            a_file.write(\",\".join([str(x) for x in readout_t]) + '\\n')\n",
    "            h_file.write(\",\".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\\n')\n",
    "            cv2.imwrite(\"logs_tetris/frame\" + str(t) + \".png\", x_t1)\n",
    "        '''\n",
    "\n",
    "def playGame():\n",
    "    sess = tf.InteractiveSession()\n",
    "    s, readout, h_fc1 = createNetwork()\n",
    "    trainNetwork(s, readout, h_fc1, sess)\n",
    "\n",
    "def main():\n",
    "    playGame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Project reference: https://github.com/yenchenlin/DeepLearningFlappyBird"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
